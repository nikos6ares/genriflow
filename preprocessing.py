{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMeWE/10s5FEsjMMILtEmWh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import numpy as np\n","import pickle\n","import librosa\n","import soundfile as sf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n","\n","def load_and_process_gtzan(\n","    gtzan_path,\n","    n_songs_per_genre=2,\n","    noise=\"weak\",\n","    n_augmentations=0,\n","    random_seed=42\n","):\n","    \"\"\"Load GTZAN dataset, extract features, apply augmentation, and save processed features.\"\"\"\n","\n","    def is_file_valid(file_path):\n","        try:\n","            with sf.SoundFile(file_path) as f:\n","                return True\n","        except Exception as e:\n","            print(f\"Corrupted file skipped: {file_path} ({e})\")\n","            return False\n","\n","    def get_target_duration(duration_sec, sample_rate=22050):\n","        return int(duration_sec * sample_rate)\n","\n","    def pad_or_trim_audio(audio_data, target_duration):\n","        if len(audio_data) < target_duration:\n","            # Pad with zeros if the audio is shorter than the target duration\n","            padding = target_duration - len(audio_data)\n","            audio_data = np.pad(audio_data, (0, padding), mode='constant')\n","        else:\n","            # Trim if the audio is longer than the target duration\n","            audio_data = audio_data[:target_duration]\n","        return audio_data\n","\n","    def extract_features(audio, sr, hop_length, label):\n","        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=8, hop_length=hop_length)\n","        mel = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=32, n_fft=2048, hop_length=hop_length)\n","        mel_db = librosa.power_to_db(mel, ref=np.max)\n","        chroma = librosa.feature.chroma_stft(y=audio, sr=sr, hop_length=hop_length)\n","        tonnetz = librosa.feature.tonnetz(y=audio, sr=sr, hop_length=hop_length)\n","\n","        return {\n","            'mfcc': mfcc,\n","            'mel_spectrogram': mel_db,\n","            'chroma': chroma,\n","            'tonnetz': tonnetz,\n","            'label': label\n","        }\n","\n","    def extract_feature_arrays(features_split):\n","        mfccs = []\n","        mel_specs = []\n","        chromas = []\n","        tonnetzs = []\n","        labels = []\n","\n","        for song_name, data in features_split.items():\n","            mfccs.append(data[\"mfcc\"])\n","            mel_specs.append(data[\"mel_spectrogram\"])\n","            chromas.append(data[\"chroma\"])\n","            tonnetzs.append(data[\"tonnetz\"])\n","            labels.append(data[\"label\"])\n","\n","        return (\n","            np.array(mfccs, dtype=object),\n","            np.array(mel_specs, dtype=object),\n","            np.array(chromas, dtype=object),\n","            np.array(tonnetzs, dtype=object),\n","            np.array(labels)\n","        )\n","\n","    def normalize_features(features):\n","        \"\"\"Normalize features across all samples and time steps.\"\"\"\n","        orig_shape = features.shape\n","        features_reshaped = features.transpose(0, 2, 1).reshape(-1, orig_shape[1])  # (samples*time, features)\n","        scaler = StandardScaler()\n","        features_normalized = scaler.fit_transform(features_reshaped)\n","        return features_normalized.reshape(orig_shape[0], orig_shape[2], orig_shape[1]).transpose(0, 2, 1)\n","\n","    # List all genres (the folder names)\n","    genres = os.listdir(gtzan_path)\n","\n","    # Initialize list to hold the file paths and corresponding genre labels\n","    songs = []\n","    labels = []\n","\n","    # Number of songs to pick per genre (40 songs from each genre)\n","    np.random.seed(random_seed)\n","    # Load songs and assign labels based on genre\n","    for genre in genres:\n","        genre_folder = os.path.join(gtzan_path, genre)\n","        if os.path.isdir(genre_folder):\n","            # List all songs in the genre folder and randomly select\n","            genre_files = [os.path.join(genre_folder, f) for f in os.listdir(genre_folder) if f.endswith('.wav')]\n","            selected_files = np.random.choice(genre_files, size=n_songs_per_genre, replace=False)\n","\n","            for file in selected_files:\n","                if is_file_valid(file):\n","                    songs.append(file)\n","                    labels.append(genre)\n","\n","    # Convert the list into a numpy array for labels\n","    songs = np.array(songs)\n","    labels = np.array(labels)\n","\n","    # First split: train vs test (already done)\n","    train_files, test_files, train_labels, test_labels = train_test_split(songs, labels, test_size=0.2, random_state=42)\n","    # Second split: train -> train + val\n","    train_files, val_files, train_labels, val_labels = train_test_split(train_files, train_labels, test_size=0.15, random_state=42)  # 80% train, 20% val\n","\n","    if noise == \"weak\":\n","        augment = Compose([\n","            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5)\n","        ])\n","    elif noise == \"strong\":\n","        augment = Compose([\n","            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.005, p=0.5),            # stronger noise\n","            TimeStretch(min_rate=0.85, max_rate=1.15, p=0.5),                            # 15% speed variation\n","            PitchShift(min_semitones=-4, max_semitones=4, p=0.5),                        # wider pitch shift\n","            Shift(min_shift=-0.3, max_shift=0.3, p=0.5),                                 # shift up to 30% of length\n","        ])\n","    else:\n","      augment = None\n","\n","    features_dict = {}\n","\n","    hop_length = 1024\n","    target_duration_sec = 30  # 30 seconds\n","    sample_rate = 44100 # default sample rat\n","    target_duration = target_duration_sec*sample_rate\n","    for split_name, file_list, label_list in zip(['train', 'val', 'test'], [train_files, val_files, test_files], [train_labels, val_labels, test_labels]):\n","        features_dict[split_name] = {}\n","\n","        for element, label in zip(file_list, label_list):\n","            audio_data, sample_rate = librosa.load(element, sr=None)\n","            audio_data = pad_or_trim_audio(audio_data, target_duration)\n","\n","            song_name = os.path.splitext(os.path.basename(element))[0]\n","            features_dict[split_name][song_name] = extract_features(audio_data, sample_rate, hop_length, label)\n","\n","            if split_name == \"train\" and n_augmentations > 0 and augment is not None:\n","              for i in range(n_augmentations):\n","                  augmented_audio = augment(samples=audio_data, sample_rate=sample_rate)\n","                  augmented_audio = pad_or_trim_audio(augmented_audio, target_duration)\n","                  features_dict[split_name][f\"{song_name}_aug{i}\"] = extract_features(augmented_audio, sample_rate, hop_length, label)\n","\n","\n","    train_mfccs_array, train_mel_array, train_chroma_array, train_tonnetz_array, y_train = extract_feature_arrays(features_dict[\"train\"])\n","    val_mfccs_array, val_mel_array, val_chroma_array, val_tonnetz_array, y_val = extract_feature_arrays(features_dict[\"val\"])\n","    test_mfccs_array, test_mel_array, test_chroma_array, test_tonnetz_array, y_test = extract_feature_arrays(features_dict[\"test\"])\n","\n","    # Print shapes\n","    print(\"Train MFCCs shape:\", train_mfccs_array.shape)\n","    print(\"Train Mel Spectrogram shape:\", train_mel_array.shape)\n","    print(\"Train Chroma shape:\", train_chroma_array.shape)\n","    print(\"Train Tonnetz shape:\", train_tonnetz_array.shape)\n","\n","    print(\"Val MFCCs shape:\", val_mfccs_array.shape)\n","    print(\"Val Mel Spectrogram shape:\", val_mel_array.shape)\n","    print(\"Val Chroma shape:\", val_chroma_array.shape)\n","    print(\"Val Tonnetz shape:\", val_tonnetz_array.shape)\n","\n","    print(\"Test MFCCs shape:\", test_mfccs_array.shape)\n","    print(\"Test Mel Spectrogram shape:\", test_mel_array.shape)\n","    print(\"Test Chroma shape:\", test_chroma_array.shape)\n","    print(\"Test Tonnetz shape:\", test_tonnetz_array.shape)\n","\n","    train_mfccs_std, test_mfccs_std, val_mfccs_std = normalize_features(train_mfccs_array), normalize_features(test_mfccs_array), normalize_features(val_mfccs_array)\n","    train_mel_std, test_mel_std, val_mel_std = normalize_features(train_mel_array), normalize_features(test_mel_array), normalize_features(val_mel_array)\n","    train_chroma_std, test_chroma_std, val_chroma_std = normalize_features(train_chroma_array), normalize_features(test_chroma_array), normalize_features(val_chroma_array)\n","    train_tonnetz_std, test_tonnetz_std, val_tonnetz_std = normalize_features(train_tonnetz_array), normalize_features(test_tonnetz_array), normalize_features(val_tonnetz_array)\n","\n","    X_train = np.concatenate([train_mfccs_std, train_mel_std, train_chroma_std, train_tonnetz_std], axis=1)\n","    X_test = np.concatenate([test_mfccs_std, test_mel_std, test_chroma_std, test_tonnetz_std], axis=1)\n","    X_val = np.concatenate([val_mfccs_std, val_mel_std, val_chroma_std, val_tonnetz_std], axis=1)\n","\n","    print(\"X_train shape:\", X_train.shape)\n","    print(\"X_test shape:\", X_test.shape)\n","    print(\"X_val shape:\", X_val.shape)\n","\n","    data_dict = {\n","        'X_train': X_train,\n","        'X_test': X_test,\n","        'X_val': X_val,\n","        'y_train': y_train,\n","        'y_test': y_test,\n","        'y_val': y_val\n","    }\n","\n","    # Save the dictionary as a .pkl file\n","    with open(\"/content/drive/MyDrive/Colab Notebooks/processed_data.pkl\", \"wb\") as f:\n","        pickle.dump(data_dict, f)\n","\n","\n","    return X_train, X_test, X_val, y_train, y_test, y_val"],"metadata":{"id":"lGMP2-TZgO-s"},"execution_count":null,"outputs":[]}]}